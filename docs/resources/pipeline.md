---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "artie_pipeline Resource - terraform-provider-artie"
subcategory: ""
description: |-
  Artie Pipeline resource. This represents a pipeline that syncs data from a single source (e.g., Postgres) to a single destination (e.g., Snowflake).
---

# artie_pipeline (Resource)

Artie Pipeline resource. This represents a pipeline that syncs data from a single source (e.g., Postgres) to a single destination (e.g., Snowflake).

## Example Usage

```terraform
resource "artie_pipeline" "postgres_to_snowflake" {
  name               = "PostgreSQL to Snowflake"
  source_reader_uuid = artie_source_reader.postgres.uuid
  tables = {
    "public.account" = {
      name                = "account"
      schema              = "public"
      enable_history_mode = true
    },
    "public.company" = {
      name   = "company"
      schema = "public"
    }
  }
  destination_connector_uuid = artie_connector.snowflake.uuid
  destination_config = {
    database = "ANALYTICS"
    schema   = "PUBLIC"
  }
  soft_delete_rows                = true
  include_artie_updated_at_column = true
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `destination_config` (Attributes) This contains configuration that pertains to the destination database but is specific to this pipeline. The basic connection settings for the destination, which can be shared by multiple pipelines, are stored in the corresponding `artie_connector` resource. (see [below for nested schema](#nestedatt--destination_config))
- `destination_connector_uuid` (String) This must point to an `artie_connector` resource that represents the destination database.
- `name` (String) The human-readable name of the pipeline. This is used only as a label and can contain any characters.
- `source_reader_uuid` (String) This must point to an `artie_source_reader` resource.
- `tables` (Attributes Map) A map of tables from the source database that you want to replicate to the destination. The key for each table should be formatted as `schema_name.table_name` if your source database uses schemas, otherwise just `table_name`. (see [below for nested schema](#nestedatt--tables))

### Optional

- `data_plane_name` (String) The name of the data plane to use for this pipeline. If this is not set, we will use the default data plane for your account. To see the full list of supported data planes on your account, click on 'New deployment' in our UI.
- `drop_deleted_columns` (Boolean) If set to true, when a column is dropped from the source it will also be dropped in the destination.
- `flush_rules` (Attributes) This contains rules for how often Artie should flush data to the destination. If not specified, Artie will provide default values. A flush will happen when any of the rules are met (e.g. 30 seconds since the last flush OR 150k rows OR 50MB of data). (see [below for nested schema](#nestedatt--flush_rules))
- `include_artie_updated_at_column` (Boolean) If set to true, Artie will add a new column to your dataset called __artie_updated_at.
- `include_database_updated_at_column` (Boolean) If set to true, Artie will add a new column to your dataset called __artie_db_updated_at.
- `soft_delete_rows` (Boolean) If set to true, a new boolean column called __artie_delete will be added to your destination to indicate if the row has been deleted.

### Read-Only

- `snowflake_eco_schedule_uuid` (String)
- `status` (String)
- `uuid` (String)

<a id="nestedatt--destination_config"></a>
### Nested Schema for `destination_config`

Optional:

- `bucket` (String) The name of the S3 bucket that data should be synced to. This should be filled if the destination is S3.
- `database` (String) The name of the database that data should be synced to in the destination. This should be filled if the destination is MS SQL or Snowflake, unless `use_same_schema_as_source` is set to true.
- `dataset` (String) The name of the dataset that data should be synced to in the destination. This should be filled if the destination is BigQuery.
- `folder` (String) If provided, all files will be stored under this folder inside the S3 bucket. This is optional and only applies if the destination is S3.
- `schema` (String) The name of the schema that data should be synced to in the destination. This should be filled if the destination is MS SQL, Redshift, or Snowflake (unless `use_same_schema_as_source` is set to true).
- `schema_name_prefix` (String) If `use_same_schema_as_source` is enabled, this prefix will be added to each schema name in the destination. This is useful if you want to namespace all of this pipeline's schemas in the destination.
- `use_same_schema_as_source` (Boolean) If set to true, each table from the source database will be synced to a schema with the same name as its source schema. This can only be used if both the source and destination support multiple schemas (e.g. PostgreSQL, Redshift, Snowflake, etc).


<a id="nestedatt--tables"></a>
### Nested Schema for `tables`

Required:

- `name` (String) The name of the table in the source database.

Optional:

- `alias` (String) An optional alias for the table. If set, this will be the name of the destination table.
- `columns_to_exclude` (List of String) An optional list of columns to exclude from syncing to the destination.
- `columns_to_hash` (List of String) An optional list of columns to hash in the destination. Values for these columns will be obscured with a one-way hash.
- `columns_to_include` (List of String) An optional list of columns to include in replication. If not provided, all columns will be replicated. A pipeline can only have one of `columns_to_include` or `columns_to_exclude` set in any of its tables.
- `enable_history_mode` (Boolean) If set to true, we will create an additional table in the destination (suffixed with `__history`) to store all changes to the source table over time.
- `individual_deployment` (Boolean) If set to true, we will spin up a separate Artie Transfer deployment to handle this table. This should only be used if this table has extremely high throughput (over 1M+ per hour) and has much higher throughput than other tables.
- `merge_predicates` (Attributes List) Optional: if the destination table is partitioned, specify the column(s) it's partitioned by. This will help with merge performance and currently only applies to Snowflake and BigQuery. For BigQuery, only one column can be specified and it must be a time column partitioned by day. (see [below for nested schema](#nestedatt--tables--merge_predicates))
- `schema` (String) The name of the schema the table belongs to in the source database. This must be specified if your source database uses schemas (such as PostgreSQL), e.g. `public`.
- `skip_deletes` (Boolean) If set to true, we will skip delete events for this table and only process insert and update events.

Read-Only:

- `is_partitioned` (Boolean)
- `uuid` (String)

<a id="nestedatt--tables--merge_predicates"></a>
### Nested Schema for `tables.merge_predicates`

Required:

- `partition_field` (String) The name of the column the destination table is partitioned by.



<a id="nestedatt--flush_rules"></a>
### Nested Schema for `flush_rules`

Optional:

- `buffer_rows` (Number) The number of rows to buffer before flushing to the destination.
- `flush_interval_seconds` (Number) The flush interval in seconds for how often Artie should flush data to the destination.
- `flush_size_kb` (Number) The size in kb of data to buffer before flushing to the destination.

## Import

Import is supported using the following syntax:

```shell
# Import a pipeline by using its UUID, which you can find in
# the URL of the deployment overview page in the Artie UI, e.g.:
# https://app.artie.com/deployments/<pipeline_uuid>/overview
terraform import artie_pipeline.my_pipeline <pipeline_uuid>

# Then print the state and copy it into your terraform config file
# (be sure to remove all read-only fields, like `uuid`, `status`, etc.):
terraform state show artie_pipeline.my_pipeline

# Then you can find the UUIDs of the objects your pipeline depends on and import those too:
terraform import artie_source_reader.my_source_reader <my_pipeline.source_reader_uuid>
terraform import artie_connector.my_source_connector <my_source_reader.connector_uuid>
terraform import artie_connector.my_destination_connector <my_pipeline.destination_connector_uuid>
```
