---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "artie_pipeline Resource - terraform-provider-artie"
subcategory: ""
description: |-
  Artie Pipeline resource. This represents a pipeline that syncs data from a single source (e.g., Postgres) to a single destination (e.g., Snowflake).
---

# artie_pipeline (Resource)

Artie Pipeline resource. This represents a pipeline that syncs data from a single source (e.g., Postgres) to a single destination (e.g., Snowflake).

## Example Usage

```terraform
resource "artie_pipeline" "postgres_to_snowflake" {
  name               = "PostgreSQL to Snowflake"
  source_reader_uuid = artie_source_reader.postgres.uuid
  tables = {
    "public.account" = {
      name                = "account"
      schema              = "public"
      enable_history_mode = true
    },
    "public.company" = {
      name   = "company"
      schema = "public"
    }
  }
  destination_connector_uuid = artie_connector.snowflake.uuid
  destination_config = {
    database = "ANALYTICS"
    schema   = "PUBLIC"
  }
  soft_delete_rows                = true
  include_artie_updated_at_column = true
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `destination_config` (Attributes) This contains configuration that pertains to the destination database but is specific to this pipeline. The basic connection settings for the destination, which can be shared by multiple pipelines, are stored in the corresponding `artie_connector` resource. (see [below for nested schema](#nestedatt--destination_config))
- `destination_connector_uuid` (String) This must point to an `artie_connector` resource that represents the destination database.
- `name` (String) The human-readable name of the pipeline. This is used only as a label and can contain any characters.
- `source_reader_uuid` (String) This must point to an `artie_source_reader` resource.
- `tables` (Attributes Map) A map of tables from the source database that you want to replicate to the destination. The key for each table should be formatted as `schema_name.table_name` if your source database uses schemas, otherwise just `table_name`. (see [below for nested schema](#nestedatt--tables))

### Optional

- `append_only` (Boolean) If set to true, data will always be appended instead of merged into the destination table. This should only be used for data that is known to be append-only (e.g. event tracking data). Rows will not be deduplicated.
- `auto_replicate_new_tables` (Boolean) If set to true, Artie will automatically start replicating any new tables that are created in the source database.
- `data_plane_name` (String) The name of the data plane to use for this pipeline. If this is not set, we will use the default data plane for your account. To see the full list of supported data planes on your account, click on 'New pipeline' in our UI.
- `default_source_schema` (String) If set, tables from this schema will not be prefixed with this schema name in the destination. Tables from other schemas will be prefixed with their source schema name to avoid table name collisions (unless `use_same_schema_as_source` is set to true). This is currently only applicable if the source is MySQL.
- `drop_deleted_columns` (Boolean) If set to true, when a column is dropped from the source it will also be dropped in the destination.
- `flush_rules` (Attributes) This contains rules for how often Artie should flush data to the destination. If not specified, Artie will provide default values. A flush will happen when any of the rules are met (e.g. 30 seconds since the last flush OR 150k rows OR 50MB of data). (see [below for nested schema](#nestedatt--flush_rules))
- `force_utc_timezone` (Boolean) If set to true, timestamps without timezone information in the source will be written as UTC in the destination.
- `include_artie_operation_column` (Boolean) If set to true, Artie will add a new column called `__artie_operation` to the destination table that indicates the operation type (insert, update, delete) for each row.
- `include_artie_updated_at_column` (Boolean) If set to true, Artie will add a new column called `__artie_updated_at` to the destination table to indicate when the row was last updated by Artie.
- `include_database_updated_at_column` (Boolean) If set to true, Artie will add a new column called `__artie_db_updated_at` to the destination table to indicate when the row was last updated by the source database.
- `include_full_source_table_name_column` (Boolean) If set to true, Artie will add a new column called `__artie_full_source_table_name` to the destination table containing the fully qualified source table name (e.g., schema.table). Useful when unifying tables across schemas/databases.
- `include_full_source_table_name_column_as_primary_key` (Boolean) If set to true, includes the full source table name column as part of the primary key in destination tables. Requires `include_full_source_table_name_column` to be true.
- `include_source_metadata_column` (Boolean) If set to true, Artie will add a new column called `__artie_source_metadata` to the destination table which will contain a JSON blob of metadata about the source event.
- `soft_delete_rows` (Boolean) If set to true, when a row is deleted from the source it will not be deleted from the destination. Instead, a new boolean column called `__artie_delete` will be added to the destination table to indicate which rows have been deleted in the source.
- `split_events_by_type` (Boolean) If set to true, Artie will split events by type and store them in separate tables. This is only applicable if the source is API.
- `staging_schema` (String) If set, Artie's temporary staging tables will be created in this schema instead of in the same schema as the destination table. This can be used to avoid cluttering the destination schema. Note: this only applies to destinations that support schemas/namespaces.
- `static_columns` (Attributes List) Static columns allow you to add hardcoded column/value pairs to all destination rows. This is useful for tagging data with metadata like environment, source identifier, etc. (see [below for nested schema](#nestedatt--static_columns))
- `write_raw_binary_values` (Boolean) If set to true, binary columns (e.g. BINARY type) are created in the destination table for raw binary data instead of creating string columns that store Base64-encoded values. It only applies when the destination is Databricks.

### Read-Only

- `snowflake_eco_schedule_uuid` (String) If the pipeline's destination is Snowflake, this can point to a Snowflake Eco Mode Schedule that will be used to adjust the pipeline's flush rules according to a schedule. This can currently only be configured via our UI.
- `uuid` (String)

<a id="nestedatt--destination_config"></a>
### Nested Schema for `destination_config`

Optional:

- `bucket` (String) The name of the S3 or GCS bucket that data should be synced to. This should be filled if the destination is S3, GCS, or Iceberg with provider `s3tables` (for Iceberg S3 Tables, this bucket is where delta files will be stored). Not used for Iceberg REST catalog.
- `create_iceberg_namespaces` (Boolean) If set to true, Artie will automatically create namespaces if they don't exist. This is only applicable if the destination is Iceberg.
- `database` (String) The name of the database that data should be synced to in the destination. This should be filled if the destination is MS SQL or Snowflake, unless `use_same_schema_as_source` is set to true.
- `dataset` (String) The name of the dataset that data should be synced to in the destination. This should be filled if the destination is BigQuery.
- `folder` (String) If provided, all files will be stored under this folder inside the S3 or GCS bucket. This is optional and only applies if the destination is S3 or GCS.
- `schema` (String) The name of the schema or namespace that data should be synced to in the destination. This should be filled if the destination is MS SQL, Redshift, Iceberg, or Snowflake (unless `use_same_schema_as_source` is set to true).
- `schema_name_prefix` (String) If `use_same_schema_as_source` is enabled, this prefix will be added to each schema name in the destination. This is useful if you want to namespace all of this pipeline's schemas in the destination.
- `table_name_separator` (String) If provided, this is the separator between database, schema and table name. This is only applicable if the destination is S3 or GCS.
- `use_same_schema_as_source` (Boolean) If set to true, each table from the source database will be synced to a schema with the same name as its source schema. This can only be used if both the source and destination support multiple schemas (e.g. PostgreSQL, Redshift, Snowflake, etc).


<a id="nestedatt--tables"></a>
### Nested Schema for `tables`

Required:

- `name` (String) The name of the table in the source database.

Optional:

- `alias` (String) An optional alias for the table. If set, this will be the name of the destination table.
- `backfill_history_table` (Boolean) If set to true, Artie will backfill the history table with existing data. This is only applicable if `enable_history_mode` is set to true.
- `columns_to_exclude` (List of String) An optional list of columns to exclude from syncing to the destination.
- `columns_to_hash` (List of String) An optional list of columns to hash in the destination. Values for these columns will be obscured with a one-way hash.
- `columns_to_include` (List of String) An optional list of columns to include in replication. If not provided, all columns will be replicated. A pipeline can only have one of `columns_to_include` or `columns_to_exclude` set in any of its tables.
- `ctid_backfill` (Boolean) If set to true, enables CTID backfill for this table. This is only applicable if the source type is `postgres`.
- `ctid_chunk_size` (Number) The chunk size to use for CTID backfill. This should be between 100,000 and 1,000,000. This is only applicable if the source type is `postgres` and `ctid_backfill` is set to true.
- `ctid_max_parallelism` (Number) The maximum parallelism for CTID backfill. This should be between 5 and 20. This is only applicable if the source type is `postgres` and `ctid_backfill` is set to true.
- `disable_replication` (Boolean) If set to true along with `enable_history_mode`, this table will only replicate to the history table and not the main destination table.
- `enable_history_mode` (Boolean) If set to true, we will create an additional table in the destination (suffixed with `__history`) to store all changes to the source table over time.
- `is_partitioned` (Boolean) If the source table is partitioned, set this to true and we will ingest data from all of its partitions. You may also need to customize `partition_suffix_regex_pattern` on the source reader.
- `merge_predicates` (Attributes List) Optional: if the destination table is partitioned, specify the partition column(s) and type. This helps merge performance and currently only applies to Snowflake and BigQuery. For BigQuery, only one column can be specified and it may be either a time-partitioned or an integer range-partitioned column; set `partition_type` to 'time' or 'integer' accordingly. (see [below for nested schema](#nestedatt--tables--merge_predicates))
- `schema` (String) The name of the schema the table belongs to in the source database. This must be specified if your source database uses schemas (such as PostgreSQL), e.g. `public`.
- `skip_deletes` (Boolean) If set to true, we will skip delete events for this table and only process insert and update events.
- `soft_partitioning` (Attributes) Optional: configuration for soft partitioning of the destination table. This can improve query performance for large tables by partitioning data based on a specified column. (see [below for nested schema](#nestedatt--tables--soft_partitioning))
- `unify_across_databases` (Boolean) If set to true, we will replicate tables with the same name and schema name from all specified databases into the same destination table. This is only applicable if the source reader has `enable_unify_across_databases` set to true and `databases_to_unify` filled.
- `unify_across_schemas` (Boolean) If set to true, we will replicate tables with the same name from all schemas into the same destination table. This is only applicable if the source reader has `enable_unify_across_schemas` set to true. You should still specify a schema name where this table exists; we will use that schema to fetch metadata for the table and validate its configuration.

Read-Only:

- `uuid` (String)

<a id="nestedatt--tables--merge_predicates"></a>
### Nested Schema for `tables.merge_predicates`

Required:

- `partition_field` (String) The name of the column the destination table is partitioned by.

Optional:

- `partition_type` (String) The type of partition to use. One of 'time' or 'integer'. Required for BigQuery.


<a id="nestedatt--tables--soft_partitioning"></a>
### Nested Schema for `tables.soft_partitioning`

Required:

- `enabled` (Boolean) Whether soft partitioning is enabled for this table.
- `max_partitions` (Number) The maximum number of partitions to maintain.
- `partition_column` (String) The column to use for soft partitioning. To prevent duplicate rows, the partition column should be immutable, for example `created_at`.
- `partition_frequency` (String) The frequency of partitioning ('monthly' and 'daily' are supported).



<a id="nestedatt--flush_rules"></a>
### Nested Schema for `flush_rules`

Optional:

- `buffer_rows` (Number) The number of rows to buffer before flushing to the destination.
- `flush_interval_seconds` (Number) The flush interval in seconds for how often Artie should flush data to the destination.
- `flush_size_kb` (Number) The size in kb of data to buffer before flushing to the destination.


<a id="nestedatt--static_columns"></a>
### Nested Schema for `static_columns`

Required:

- `column` (String) The name of the column to add to the destination table.
- `value` (String) The static value to populate for this column in all rows.

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
# Import a pipeline by using its UUID, which you can find by:
# 1. Go to the pipeline overview page in the Artie UI
# 2. Open the dropdown in the top right corner
# 3. Select "View UUIDs" to see all related resource UUIDs
terraform import artie_pipeline.my_pipeline <pipeline_uuid>

# Then print the state and copy it into your terraform config file
# (be sure to remove all read-only fields, like `uuid`):
terraform state show artie_pipeline.my_pipeline
```
